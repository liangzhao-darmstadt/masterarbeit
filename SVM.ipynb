{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyreadr as py\n",
    "import missingno as msno\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os_windows: True\n",
      "os_ubuntu: False\n",
      "sys.version: 3.7.1 (default, Oct 28 2018, 08:39:03) [MSC v.1912 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "\n",
    "\n",
    "os_windows = False\n",
    "os_ubuntu = False\n",
    "if platform.platform() == \"Linux-5.4.0-58-generic-x86_64-with-glibc2.10\":\n",
    "    os_ubuntu = True\n",
    "else:\n",
    "    os_windows = True\n",
    "print(\"os_windows: \" + str(os_windows))\n",
    "print(\"os_ubuntu: \" + str(os_ubuntu))\n",
    "\n",
    "print(\"sys.version: \" + str(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.__version__: 0.23.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "\n",
    "print(\"sklearn.__version__: \" + str(sklearn.__version__))\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - define helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_data(data, eta=2):\n",
    "    data_normalized = np.copy(data)\n",
    "    shape0 = data.shape[0]\n",
    "    shape1 = data.shape[1]\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    for i in range(shape1):\n",
    "        for j in range(shape0):\n",
    "            data_normalized[j, i] = (data[j, i] - mean[i]) / (std[i] * eta)\n",
    "    return data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix the fault free data and faulty data\n",
    "def mix_shuffle_data(fault_free_data, faulty_data):\n",
    "\n",
    "    fault_free_label = np.zeros((fault_free_data.shape[0], 1))\n",
    "    faulty_label = np.ones((faulty_data.shape[0], 1))\n",
    "\n",
    "    a1 = np.hstack((fault_free_label, fault_free_data))\n",
    "    a2 = np.hstack((faulty_label, faulty_data))\n",
    "    mixed_data = np.vstack((a1, a2))\n",
    "    np.random.shuffle(mixed_data)\n",
    "    return mixed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_error_classifier(prediction, label):\n",
    "    detection_rate = prediction.sum() / prediction.shape[0]\n",
    "    s1 = label + str(detection_rate)\n",
    "\n",
    "    print(s1)\n",
    "    return detection_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os_windows:\n",
    "    root_folder = \"C:/Users/liang/Masterarbeit_Jupyter_Lab/\"\n",
    "else:\n",
    "    root_folder = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta: 2\n"
     ]
    }
   ],
   "source": [
    "eta = 2\n",
    "print(\"eta: \" + str(eta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data, normalize_data = True, True\n",
    "\n",
    "if read_data and normalize_data:\n",
    "    fault_free_train_samplesize20000 = pd.read_csv(\n",
    "        root_folder + \"data_set_csv/fault_free_train_sample_size=20000\", sep=\",\", header=None,\n",
    "    ).to_numpy()\n",
    "    fault_free_train_samplesize40000 = pd.read_csv(\n",
    "        root_folder + \"data_set_csv/fault_free_train_sample_size=40000\", sep=\",\", header=None,\n",
    "    ).to_numpy()\n",
    "    fault_free_train_samplesize100000 = pd.read_csv(\n",
    "        root_folder + \"data_set_csv/fault_free_train_sample_size=100000\", sep=\",\", header=None,\n",
    "    ).to_numpy()\n",
    "    fault_free_train_samplesize250000 = pd.read_csv(\n",
    "        root_folder + \"data_set_csv/fault_free_train_sample_size=250000\", sep=\",\", header=None,\n",
    "    ).to_numpy()\n",
    "\n",
    "    fault_free_train_samplesize20000_normalized = normalization_data(fault_free_train_samplesize20000, eta)\n",
    "    fault_free_train_samplesize40000_normalized = normalization_data(fault_free_train_samplesize40000, eta)\n",
    "    fault_free_train_samplesize100000_normalized = normalization_data(fault_free_train_samplesize100000, eta)\n",
    "    fault_free_train_samplesize250000_normalized = normalization_data(fault_free_train_samplesize250000, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 52)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fault_free_test1 = pd.read_csv(root_folder + \"data_set_csv/fault_free_test1=20000\", sep=\",\", header=None).to_numpy()\n",
    "fault_free_test1_normalized = normalization_data(fault_free_test1, eta)\n",
    "fault_free_test1_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 52)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fault_free_test2 = pd.read_csv(root_folder + \"data_set_csv/fault_free_test2=20000\", sep=\",\", header=None).to_numpy()\n",
    "fault_free_test2_normalized = normalization_data(fault_free_test2, eta)\n",
    "fault_free_test2_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470400, 52)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faulty_train = pd.read_csv(root_folder + \"data_set_csv/faulty_train=10%\", sep=\",\", header=None).to_numpy()\n",
    "faulty_train_normalized = normalization_data(faulty_train, eta)\n",
    "faulty_train_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784000, 52)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faulty_test = pd.read_csv(root_folder + \"data_set_csv/faulty_test=10%\", sep=\",\", header=None).to_numpy()\n",
    "faulty_test_normalized = normalization_data(faulty_test, eta)\n",
    "faulty_test_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(650000, 53)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fault_free_train = fault_free_train_samplesize250000\n",
    "fault_free_train_normalized = fault_free_train_samplesize250000_normalized\n",
    "faulty_train_sample_size = 400000\n",
    "\n",
    "train_data = mix_shuffle_data(fault_free_train, faulty_train[:faulty_train_sample_size, :])\n",
    "train_data_normalized = mix_shuffle_data(\n",
    "    fault_free_train_normalized, faulty_train_normalized[:faulty_train_sample_size, :]\n",
    ")\n",
    "\n",
    "train_data.shape\n",
    "train_data_normalized.shape\n",
    "# np.random.shuffle(train_data)\n",
    "# np.random.shuffle(train_data_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - set model parameters using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "# model = RandomForestClassifier()\n",
    "# evaluate the model\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "sample_size_start = 0\n",
    "sample_size_end = 5000\n",
    "\n",
    "set_normalized = True\n",
    "set_grid_search = True\n",
    "\n",
    "if set_normalized:\n",
    "    X = train_data_normalized[sample_size_start:sample_size_end, 1:]\n",
    "    y = train_data_normalized[sample_size_start:sample_size_end, 0]\n",
    "else:\n",
    "    X = train_data[sample_size_start:sample_size_end, 1:]\n",
    "    y = train_data[sample_size_start:sample_size_end, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier = svm.SVC(kernel='rbf', C=1).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if set_grid_search:\n",
    "#     # model fitting and hyperparameter tunning using gridsearch\n",
    "#     test_RF_classifier = RandomForestClassifier()\n",
    "#     # weights = np.linspace(0.05, 0.95, 20)\n",
    "#     prams = {\n",
    "#         \"n_estimators\": [100, 200, 500],\n",
    "#         \"max_depth\": [15, 20, 25, 30, 35],  # ,'class_weight': [{0: x, 1: 1.0-x} for x in weights]\n",
    "#     }\n",
    "#     model = GridSearchCV(test_RF_classifier, param_grid=prams, verbose=10, n_jobs=-1, scoring=\"accuracy\", cv=3)\n",
    "#     model.fit(X, y)\n",
    "#     print(\"Best estimator is\", model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth=model.best_params_.get('max_depth')\n",
    "# n_estimators=model.best_params_.get('n_estimators')\n",
    "\n",
    "# RF_classifier = RandomForestClassifier(n_jobs=-1, max_depth=max_depth, n_estimators=n_estimators)\n",
    "# RF_classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fault_free_train_samplesize20000_normalized_pred: 0.0191\n",
      "fault_free_test1_normalized_pred: 0.01635\n",
      "fault_free_test2_normalized_pred: 0.01615\n",
      "faulty_train_normalized_pred: 0.9668771258503401\n",
      "faulty_test_normalized_pred: 0.9543686224489796\n"
     ]
    }
   ],
   "source": [
    "classifier = svc_classifier\n",
    "if set_normalized:\n",
    "    fault_free_train_samplesize20000_normalized_pred = classifier.predict(fault_free_train_samplesize20000_normalized)\n",
    "    fault_free_test1_normalized_pred = classifier.predict(fault_free_test1_normalized)\n",
    "    fault_free_test2_normalized_pred = classifier.predict(fault_free_test2_normalized)\n",
    "    faulty_train_normalized_pred = classifier.predict(faulty_train_normalized)\n",
    "    faulty_test_normalized_pred = classifier.predict(faulty_test_normalized)\n",
    "    pred_error_classifier(fault_free_train_samplesize20000_normalized_pred, \"fault_free_train_samplesize20000_normalized_pred: \")\n",
    "    pred_error_classifier(fault_free_test1_normalized_pred, \"fault_free_test1_normalized_pred: \")\n",
    "    pred_error_classifier(fault_free_test2_normalized_pred, \"fault_free_test2_normalized_pred: \")\n",
    "    pred_error_classifier(faulty_train_normalized_pred, \"faulty_train_normalized_pred: \")\n",
    "    pred_error_classifier(faulty_test_normalized_pred, \"faulty_test_normalized_pred: \")\n",
    "else:\n",
    "    fault_free_train_samplesize20000_pred = classifier.predict(fault_free_train_samplesize20000)\n",
    "    fault_free_test1_pred = classifier.predict(fault_free_test1)\n",
    "    fault_free_test2_pred = classifier.predict(fault_free_test2)\n",
    "    faulty_train_pred = classifier.predict(faulty_train)\n",
    "    faulty_test_pred = classifier.predict(faulty_test)\n",
    "    pred_error_classifier(fault_free_train_samplesize20000_pred, \"fault_free_train_samplesize20000: \")\n",
    "    pred_error_classifier(fault_free_test1_pred, \"fault_free_test1: \")\n",
    "    pred_error_classifier(fault_free_test2_pred, \"fault_free_test2: \")\n",
    "    pred_error_classifier(faulty_train_pred, \"faulty_train: \")\n",
    "    pred_error_classifier(faulty_test_pred, \"faulty_test: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
